name: cpj-lifecycle

on:
  workflow_dispatch:
    inputs:

      labName:
        description: The name of the lab
        required: true
        type: string
        default: 'mva'

      labNumberParticipants:
        description: The number of participants in the lab
        required: true
        type: string
        default: '16'

      nodeCount:
        description: The number of nodes to create
        required: true
        type: string
        default: '3'
        
      k8sVersion:
        description: Default k8s version
        required: true
        type: string
        default: '1.21.9'

env:
  TF_VAR_labname: ${{ github.event.inputs.labName }}
  TF_VAR_labNumberParticipants: ${{ github.event.inputs.labNumberParticipants }}
  TF_VAR_nodecount: ${{ github.event.inputs.nodeCount }}
  TF_VAR_k8sVersion: ${{ github.event.inputs.k8sVersion }}

  # provision hosting
  hugoVersion: '0.76.1'
  ZIPPASS: 'cpjsecretpassword'

  # configure Participant-Pods
  AKSNAME: ${{ github.event.inputs.labName}}aks'
  TF_VAR_participantPodName: 'participant-pod-statefulset'
  TF_VAR_participantPodLabel: 'participant-pod'
  TF_VAR_participantPodNamespaceName: 'user'
  TF_VAR_participantPodDockerUser: 'novatec'
  TF_VAR_sshUserPw: 'CPJSchulung2022'

jobs:
  terraform-plan:
    uses: NovatecConsulting/technologyconsulting-cpj-infrastructure/.github/workflows/cpj-terraform-reusable.yaml@main
    with:
      TF_ACTION: 'plan'
      TF_VAR_labname: ${{ github.event.inputs.labName }}
      TF_VAR_labNumberParticipants: ${{ github.event.inputs.labNumberParticipants }}
      TF_VAR_nodecount: ${{ github.event.inputs.nodeCount }}
      TF_VAR_k8sVersion: ${{ github.event.inputs.k8sVersion }}
    secrets:
      TF_VAR_ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      TF_VAR_STATE_BLOBACCESSKEY: ${{ secrets.STATE_BLOBACCESSKEY }}

  terraform-apply:
    needs: terraform-plan
    uses: NovatecConsulting/technologyconsulting-cpj-infrastructure/.github/workflows/cpj-terraform-reusable.yaml@main
    with:
      TF_ACTION: 'apply'
      TF_OPTIONS: '-auto-approve'
      TF_VAR_labname: ${{ github.event.inputs.labName }}
      TF_VAR_labNumberParticipants: ${{ github.event.inputs.labNumberParticipants }}
      TF_VAR_nodecount: ${{ github.event.inputs.nodeCount }}
      TF_VAR_k8sVersion: ${{ github.event.inputs.k8sVersion }}
    secrets:
      TF_VAR_ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      TF_VAR_STATE_BLOBACCESSKEY: ${{ secrets.STATE_BLOBACCESSKEY }}
    
  terraform-destroy:
    needs: terraform-plan
    uses: NovatecConsulting/technologyconsulting-cpj-infrastructure/.github/workflows/cpj-terraform-reusable.yaml@main
    with:
      environment: 'automatic-tf-destroy'
      TF_ACTION: 'destroy'
      TF_OPTIONS: '-auto-approve'
      TF_VAR_labname: ${{ github.event.inputs.labName }}
    secrets:
      TF_VAR_ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      TF_VAR_STATE_BLOBACCESSKEY: ${{ secrets.STATE_BLOBACCESSKEY }}
      AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}
      
  hugo-hosting:
    needs: terraform-apply
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Download Hugo
        run: |
          wget -c 'https://github.com/gohugoio/hugo/releases/download/v${{ env.hugoVersion }}/hugo_extended_${{ env.hugoVersion }}_Linux-64bit.deb'

      - name: Install Hugo
        run: |
          sudo dpkg -i hugo_extended_${{ env.hugoVersion }}_Linux-64bit.deb

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Checkout excercises
        uses: actions/checkout@master
        with:
          repository: NovatecConsulting/technologyconsulting-cpj-excercises
          ref: 'main'
          token: ${{ secrets.GH_PAT }}
          path: './hugo'

      - name: run hugo hosting shell script

        run: |
          #!/bin/bash
          set -xe

          cd ./hugo
        
          pwd
          echo 'add Pod-Service data into hugofiles'

          echo "<h2>IP Addressen</h2>" >> content/_index.md

          AKSNAME="${{ env.TF_VAR_labname }}aks"

          az aks get-credentials -g ${{ env.TF_VAR_labname }} -n $AKSNAME

          echo "Get Participant Pods Service IPs"

          kubectl get svc

          printf "<h3>Webssh-Server Loadbalancer IP </h3>" >>  content/_index.md

          printf "<p>" >>  content/_index.md
          kubectl describe svc webssh-server | grep Ingress >>  content/_index.md
          printf "</p>" >>  content/_index.md
          printf "<br/>" >>  content/_index.md

          printf "<h3>Pod-Service Loadbalancer IPs </h3>" >>  content/_index.md

          for ((i = i; i<$TF_VAR_labNumberParticipants; i++)); do
            printf "<h5>Pod $i Service ClusterIP: </h5>" >>  content/_index.md

            printf "<p>" >>  content/_index.md
            kubectl describe svc "${{ env.TF_VAR_participantPodLabel }}-$i-service" | grep Ingress >>  content/_index.md
            printf "</p>" >>  content/_index.md

            printf "<p>" >>  content/_index.md
            kubectl describe svc "${{ env.TF_VAR_participantPodLabel }}-$i-service" | grep IPs >>  content/_index.md
            printf "</p>" >>  content/_index.md
            printf "<br/>" >>  content/_index.md
          done


          echo 'creating hugo static page'
          mkdir themes && cd themes && git clone https://github.com/matcornic/hugo-theme-learn.git && cd ..
          # TC-380: update clipboards.js to avoid jumping in page on copy-to-clipboard when focus was lost
          curl -sSL https://raw.githubusercontent.com/zenorocha/clipboard.js/master/dist/clipboard.min.js > themes/hugo-theme-learn/static/js/clipboard.min.js

          cp config.toml localZIPgenConfig.toml

          sed -i 's/relativeURLs = "false"/relativeURLs = "true"/g' localZIPgenConfig.toml
          sed -i 's/uglyURLs = "false"/uglyURLs = "true"/g' localZIPgenConfig.toml
          sed -i 's/publishDir = "public"/publishDir = "CPJ"/g' localZIPgenConfig.toml
          sed -i 's/baseURL = "\/"/baseURL = ""/g' localZIPgenConfig.toml
          sed -i 's/HUGO\_BASEURL = "https\:\/\/cloudacademyhugo.z6.web.core.windows.net\/"/HUGO_BASEURL = ""/g' localZIPgenConfig.toml

          cat localZIPgenConfig.toml
          echo ""
          echo "create zip folder"

          sudo hugo --config localZIPgenConfig.toml
          ls CPJ/

          echo "zip pass var is:"
          echo ${{ env.ZIPPASS }}
          zip -rP ${{ env.ZIPPASS }} cloud_platform_journey-materials.zip CPJ/
          cp cloud_platform_journey-materials.zip content/_index.files/
          sudo hugo --log -v
          ls public/

          az storage blob upload-batch -d \$web -s ./public --account-name "${{ env.TF_VAR_labname }}materials"

  configure-participant-pods:
    needs: terraform-apply
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: executing shell script
      run: |
        #!/bin/bash
        set -xe

        AKSNAME="${{ env.TF_VAR_labname }}aks"
        TF_VAR_labname="${{ env.TF_VAR_labname }}"
        TF_VAR_sshUserPw="${{ env.TF_VAR_sshUserPw }}"
        TF_VAR_labNumberParticipants="${{ env.TF_VAR_labNumberParticipants }}"
        TF_VAR_participantPodName="${{ env.TF_VAR_participantPodName }}"
        TF_VAR_participantPodLabel="${{ env.TF_VAR_participantPodLabel }}"
        TF_VAR_participantPodNamespaceName="${{ env.TF_VAR_participantPodNamespaceName }}"
        TF_VAR_participantPodDockerUser="${{ env.TF_VAR_participantPodDockerUser }}"




        az aks get-credentials -g ${TF_VAR_labname} -n $AKSNAME

        echo "Configure Kubectl in Participant Pods"

        kubectl get pods

        echo "Testing Sysbox-Daemonset"
        until kubectl get daemonsets sysbox-deploy-k8s -n kube-system -o jsonpath="{.status.numberReady}" | grep -v --silent 0; do
          kubectl describe daemonsets sysbox-deploy-k8s -n kube-system
          kubectl get daemonsets sysbox-deploy-k8s -n kube-system

          date
          echo "No Worker-Node with Sysbox-Daemonset ready. Waiting 30s"
          sleep 30
        done
        echo "Testing Sysbox-Daemonset Success"

        echo "Testing Participant-Pod"
        until kubectl get statefulSets $TF_VAR_participantPodName -o jsonpath="{.status.readyReplicas}" | grep -q $TF_VAR_labNumberParticipants; do
          kubectl get statefulSets $TF_VAR_participantPodName -o jsonpath="{.status.readyReplicas}" | grep -v 0
          kubectl get pods

          date
          echo "Not all Participant-Pod are ready. Waiting 10s"
          sleep 10
        done
        echo "Testing Participant-Pod Success"

        echo "Configure Participant-Pod"

        USER_NOVATEC_DIR=home/$TF_VAR_participantPodDockerUser
        USER_ROOT_DIR=/root

        CONTEXT="$(kubectl config current-context)"
        printf "$CONTEXT \n"

        CLUSTER_NAME="$(kubectl config get-contexts "$CONTEXT" | awk '{print $3}' | tail -n 1)"
        printf "$CLUSTER_NAME \n"

        ENDPOINT="$(kubectl config view -o jsonpath="{.clusters[?(@.name == \"${CLUSTER_NAME}\")].cluster.server}")"
        printf "$ENDPOINT \n"

        for ((c = 0; c < $TF_VAR_labNumberParticipants; c++)); do
          echo "Iteration: $c"

          printf "\nConfigure the Kubernetes cluster inside the pod '$TF_VAR_participantPodName-$c' so that the pod can access it via kubectl.... \n"
          
          SERVICE_ACCOUNT_NAME=$TF_VAR_participantPodNamespaceName-$c-sa
          printf "$SERVICE_ACCOUNT_NAME \n"

          NAMESPACE=$TF_VAR_participantPodNamespaceName-$c
          printf "$NAMESPACE \n"

          SECRET_NAME="$(kubectl get sa $SERVICE_ACCOUNT_NAME --namespace=$NAMESPACE -o json | jq -r .secrets[].name)"
          printf "$SECRET_NAME \n"

          kubectl get secret --namespace=$NAMESPACE $SECRET_NAME -o json | jq -r '.data["ca.crt"]' | base64 -di > participant-pod/ca$c.crt

          USER_TOKEN="$(kubectl get secret --namespace $NAMESPACE $SECRET_NAME -o json | jq -r '.data["token"]' | base64 -di)"
          printf "$USER_TOKEN \n"


          kubectl cp participant-pod/ca$c.crt $TF_VAR_participantPodName-$c:$USER_ROOT_DIR

          rm participant-pod/ca$c.crt

          kubectl cp participant-pod/example-deployments $TF_VAR_participantPodName-$c:$USER_ROOT_DIR
          kubectl cp participant-pod/configs/.bashrc $TF_VAR_participantPodName-$c:$USER_ROOT_DIR


          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(kubectl config set-cluster "${CLUSTER_NAME}" --server="${ENDPOINT}" --certificate-authority="${USER_ROOT_DIR}/ca${c}".crt --embed-certs=true)"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(kubectl config set-credentials "${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}" --token="${USER_TOKEN}")"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(kubectl config set-context "${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}" --cluster="${CLUSTER_NAME}" --user="${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}" --namespace="${NAMESPACE}")"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(kubectl config use-context "${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}")"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(echo  $'\nKUBECONFIG=${USER_ROOT_DIR}/.kube/config' >> "${USER_ROOT_DIR}"/.bashrc)"

          echo "Configure Novatec-User \n"

          kubectl cp participant-pod/example-deployments $TF_VAR_participantPodName-$c:$USER_NOVATEC_DIR
          kubectl cp participant-pod/configs/.bash_profile $TF_VAR_participantPodName-$c:$USER_NOVATEC_DIR
          kubectl cp participant-pod/configs/.bashrc $TF_VAR_participantPodName-$c:$USER_NOVATEC_DIR


          # Change PW of Novatec-User
          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(echo "${TF_VAR_participantPodDockerUser}:${TF_VAR_sshUserPw}" | chpasswd)"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(mkdir -p ${USER_NOVATEC_DIR}/.kube)"
          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(cp -r "${USER_ROOT_DIR}"/.kube/config "${USER_NOVATEC_DIR}"/.kube)"
          # kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(chown -R $TF_VAR_participantPodDockerUser "${USER_NOVATEC_DIR}"/.kube/)"

          kubectl exec -it $TF_VAR_participantPodName-$c -- bash -c "(echo  $'\nKUBECONFIG=${USER_NOVATEC_DIR}/.kube/config' >> "${USER_NOVATEC_DIR}/.bashrc")"

        done
      working-directory: 'infrastructureAsCode'
  servicemesh:
    environment: trainer-approval-needed-for-servicemesh
    needs: [configure-participant-pods]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: run servicemesh script
        run: |
          #!/bin/bash
          set -xe
          AKSNAME="${{ env.TF_VAR_labname }}aks"
          COUNTMAX="${{ env.TF_VAR_labNumberParticipants }}"
          #first set aks creds:
          az aks get-credentials -g ${{ env.TF_VAR_labname }} -n $AKSNAME
          sudo curl --silent --location https://github.com/linkerd/linkerd2/releases/download/stable-2.10.2/linkerd2-cli-stable-2.10.2-linux-amd64 --output /usr/local/bin/linkerd
          sudo chmod +x /usr/local/bin/linkerd
          linkerd version
          if linkerd check --pre; then
              linkerd install | kubectl apply -f -
              linkerd check --wait 15m0s
              linkerd viz install | kubectl apply -f -
          else
              echo "WARNING: Skipping linkerd install due to failed pre check. This might mean linkerd was already installed in the cluster, or something else, so please investigate by yourself."
          fi
          linkerd check --wait 15m0s
          kubectl get deployments --namespace ingress-nginx -o yaml | linkerd inject --ingress - | kubectl apply -f -
          kubectl get deployments --namespace traefik-v2 -o yaml | linkerd inject --ingress - | kubectl apply -f -
          #prepare istio in version 1.7.0
          wget https://github.com/istio/istio/releases/download/1.7.0/istioctl-1.7.0-linux-amd64.tar.gz
          tar xzvpf istioctl-1.7.0-linux-amd64.tar.gz 
          sudo mv istioctl /usr/local/bin
          istioctl install --set profile=default --set meshConfig.accessLogFile=/dev/stdout
          # install add-ons (there may be some timing issues which will be resolved when the command is run again)
          kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.7.0/samples/addons/prometheus.yaml
          kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.7.0/samples/addons/grafana.yaml
          kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.7.0/samples/addons/jaeger.yaml
          kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.7.0/samples/addons/kiali.yaml ||:
          sleep 60
          kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.7.0/samples/addons/kiali.yaml 
          #RBAC:
          echo "we need to extend perms with cluster-roles of service from all vms"
          for ((i = 0 ; i < ${COUNTMAX} ; i++)); do
            echo "VM#: $i"
          cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: ${{ env.TF_VAR_labname }}-vm-${i}-cluster-role
          rules:
          - apiGroups: ["", "extensions","apps","autoscaling","batch","apiextensions.k8s.io","acid.zalan.do","elasticsearch.k8s.elastic.co","kibana.k8s.elastic.co","metrics.k8s.io","linkerd.io","tap.linkerd.io","networking.istio.io","security.istio.io","rbac.authorization.k8s.io","admissionregistration.k8s.io","policy","apiregistration.k8s.io","monitoring.coreos.com"]
            resources: ["*"]
            verbs: ["get", "list", "watch","create","patch"]
          EOF
          done
  observability:
    environment: trainer-approval-needed-for-observability
    needs: [configure-participant-pods]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: run observability script
        working-directory: configurationAsCode
        run: |
          #!/bin/bash
          set -xe
          #Because its to heavy for terraform to perfom these steps. 
          #This script is used for AKS provisioning regarding to the observability exercises
          AKSNAME="${{ env.TF_VAR_labname }}aks"
          COUNTMAX="${{ env.TF_VAR_labNumberParticipants }}"
          #first set aks creds:
          az aks get-credentials -g ${TF_VAR_labname} -n $AKSNAME
          # in each covered namespace, what should be checked via plain TCP connect (port must be stated!)
          BLACKBOX_TCP='
              todoui:8090
              todobackend:8080
              postgresdb:5432
          '
          # which dashboards should be added to Grafana
          declare -A DASHBOARDS
          DASHBOARDS['ocelot-self']='https://grafana.com/api/dashboards/10140/revisions/6/download'            # https://grafana.com/grafana/dashboards/10140/revisions
          DASHBOARDS['ocelot-jvm']='https://grafana.com/api/dashboards/9598/revisions/4/download'              # https://grafana.com/grafana/dashboards/9598/revisions
          DASHBOARDS['ocelot-gc']='https://grafana.com/api/dashboards/12162/revisions/2/download'              # https://grafana.com/grafana/dashboards/12162/revisions
          DASHBOARDS['ocelot-http']='https://grafana.com/api/dashboards/10138/revisions/3/download'            # https://grafana.com/grafana/dashboards/10138/revisions
          DASHBOARDS['ocelot-service']='https://grafana.com/api/dashboards/10139/revisions/4/download'         # https://grafana.com/grafana/dashboards/10139/revisions
          DASHBOARDS['postgres-database']='https://grafana.com/api/dashboards/9628/revisions/5/download'       # https://grafana.com/grafana/dashboards/9628/revisions
          DASHBOARDS['alerts-overview']='https://grafana.com/api/dashboards/4181/revisions/2/download'         # https://grafana.com/grafana/dashboards/4181/revisions
          DASHBOARDS['alertmanager']='https://grafana.com/api/dashboards/9578/revisions/4/download'            # https://grafana.com/grafana/dashboards/9578/revisions
          DASHBOARDS['jaeger']='https://grafana.com/api/dashboards/10001/revisions/2/download'                 # https://grafana.com/grafana/dashboards/10001/revisions
          DASHBOARDS['k8s-storage']='https://grafana.com/api/dashboards/11455/revisions/6/download'            # https://grafana.com/grafana/dashboards/11455/revisions
          DASHBOARDS['node-exporter-full']='https://grafana.com/api/dashboards/1860/revisions/21/download'     # https://grafana.com/grafana/dashboards/1860/revisions
          DASHBOARDS['blackbox']='https://grafana.com/api/dashboards/5345/revisions/3/download'                # https://grafana.com/grafana/dashboards/5345/revisions
          # jmx-overview https://grafana.com/grafana/dashboards/3457 # defaults to job='jmx' and metrics as scraped from https://github.com/prometheus/jmx_exporter
          # jmx-prometheus-exporter https://grafana.com/grafana/dashboards/7727 # same as above for metrics
          # Stuff for prometheus
          KUBERNETES_SERVERVERSION="$(kubectl version -o json | jq '.serverVersion')"
          MAJOR="$(echo "$KUBERNETES_SERVERVERSION" | jq '.major' | tr -d '"')"
          MINOR="$(echo "$KUBERNETES_SERVERVERSION" | jq '.minor' | tr -d '"')"
          echo "Comparing cluster to https://github.com/prometheus-operator/kube-prometheus#kubernetes-compatibility-matrix ..."
          if [[ "$MAJOR" -eq 1 ]] && [[ "$MINOR" -ge 20 ]] && [[ "$MINOR" -le 21 ]]; then
              echo "Compatible, now continuing ..."
          else
              echo "ERROR: cluster version incompatible: $KUBERNETES_SERVERVERSION"
              echo "You first need to adjust the kube-prometheus release and retest, so exiting now."
              exit 1
          fi
          rm -rf kube-prometheus
          # heed https://github.com/prometheus-operator/kube-prometheus#kubernetes-compatibility-matrix
          git clone --branch release-0.8 https://github.com/prometheus-operator/kube-prometheus
          cd kube-prometheus
          # prometheus-operator needs more memory in bigger setups
          sed -i -e 's#memory: 200Mi#memory: 800Mi#' -e 's#memory: 100Mi#memory: 400Mi#' manifests/setup/prometheus-operator-deployment.yaml
          # uses its own namespace "monitoring", first creating the base structures; --force might be required during upgrades
          kubectl apply --force -f manifests/setup/
          # wait until CRD are fully in place
          until kubectl get servicemonitors --all-namespaces ; do sleep 1; date; done
          # ad-hoc limit resource usage at the cost of availability
          sed -i -e 's#^  replicas: .*#  replicas: 1#' manifests/prometheus-prometheus.yaml
          # default retention is only 24h (set to "h" as alertmanager doesn't know "d")
          sed -i -e 's#^  replicas: .*$#\0\n  retention: "336h"#' manifests/alertmanager-alertmanager.yaml manifests/prometheus-prometheus.yaml
          # persist storage ad-hoc (alternatively Thanos could be used, as utilizing the PVC requires extended privileges)
          sed -i -e 's#^  retention: .*$#\0\n  storage:\n    volumeClaimTemplate:\n      apiVersion: "v1"\n      kind: "PersistentVolumeClaim"\n      spec:\n        accessModes: ["ReadWriteOnce"]\n        resources: { requests: { storage: "20Gi" } }#' manifests/prometheus-prometheus.yaml
          sed -i -e 's#^    runAsNonRoot: .*#    runAsNonRoot: false#' -e 's#^    runAsUser: .*#    runAsUser: 0\n    runAsGroup: 0#' manifests/prometheus-prometheus.yaml
          # provide additional scrape configuration (later used for blackbox and various statically listed exporters)
          sed -i -e 's#^  alerting:$#  additionalScrapeConfigs:\n    key: prometheus-additional.yaml\n    name: additional-scrape-configs\n\0#' manifests/prometheus-prometheus.yaml
          # some additional alerting rules
          cat ../observability/prometheus-rules.yaml >> manifests/prometheus-rules.yaml
          # ad-hoc remove rules that won't make sense in our context
          sed -i -e '/^  - name: kubernetes-system-scheduler$/,+23d' manifests/kubernetes-prometheusRule.yaml
          # change alert grouping
          sed -z -i -e 's#      "group_by":\n      - "namespace"#      "group_by":\n      - "alertname"\n      - "cluster"\n      - "service"#' manifests/alertmanager-secret.yaml
          # needs more resources in bigger setups
          sed -i -e 's#cpu: ...m#cpu: 500m#' -e 's#cpu: .0m#cpu: 100m#' manifests/node-exporter-daemonset.yaml
          # adjust blackbox exporter config
          cat <<.EOF > manifests/blackbox-exporter-configuration.yaml.modules
                "kube-api":
                  "http":
                    "method": "GET"
                    "no_follow_redirects": false
                    "preferred_ip_protocol": "ip4"
                    "tls_config":
                      "insecure_skip_verify": false
                      "ca_file": "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
                    "bearer_token_file": "/var/run/secrets/kubernetes.io/serviceaccount/token"
                    "valid_http_versions":
                    - "HTTP/2.0"
                    "valid_status_codes": []
                  "prober": "http"
                  "timeout": "5s"
          .EOF
          sed -i -e '/^    "modules":$/r manifests/blackbox-exporter-configuration.yaml.modules' manifests/blackbox-exporter-configuration.yaml
          # Grafana base config
          # ad-hoc ensure all referenced plugins are present, cf. https://github.com/prometheus-operator/kube-prometheus/issues/305, hardcode sdg for now
          sed -i -e 's#^        name: grafana$#\0\n        env:\n        - name: GF_INSTALL_PLUGINS\n          value: "grafana-piechart-panel,novatec-sdg-panel 2.3.0"#' manifests/grafana-deployment.yaml
          # no information disclosure
          sed -i -e 's#^        - name: GF_INSTALL_PLUGINS$#        - name: GF_ANALYTICS_REPORTING_ENABLED\n          value: "false"\n\0#' manifests/grafana-deployment.yaml
          # needs more resources in bigger setups
          sed -i -e 's#cpu: .00m#cpu: 500m#' -e 's#memory: 200Mi#memory: 400Mi#' -e 's#memory: 100Mi#memory: 200Mi#' manifests/grafana-deployment.yaml
          # ad-hoc add some dashboards
          for key in "${!DASHBOARDS[@]}"; do
              echo "$key : ${DASHBOARDS[$key]}"
              sed -i \
                  -e "s#^        - mountPath: /grafana-dashboard-definitions/0/apiserver\$#        - mountPath: /grafana-dashboard-definitions/0/$key\n          name: grafana-dashboard-$key\n          readOnly: false\n\0#" \
                  -e "s#^        name: grafana-dashboard-workload-total\$#\0\n      - configMap:\n          name: grafana-dashboard-$key\n        name: grafana-dashboard-$key#" \
                  manifests/grafana-deployment.yaml
              # override some Ocelot dashboards to provide versions that allow filtering by namespace
              FILENAME="../observability/$key.json"
              if ! [ -f "$FILENAME" ]; then
                  FILENAME="$key.json"
                  curl --silent "${DASHBOARDS[$key]}" | \
                      sed \
                      -e 's#\${DS_PROMETHEUS}#prometheus#g' \
                      -e 's#\${DS_PROMETHEUS-APL}#prometheus#g' \
                      -e 's#\${DS_LOCALPROMETHEUS}#prometheus#g' \
                      -e 's#\${DS_PRODUCTION-AU}#prometheus#g' \
                      > $FILENAME
                  # fix panel reference in service dashboard
                  [ "$key" = 'ocelot-service' ] && sed -i -e 's#"version": "2.0"#"version": "2.3.0"#' -e 's#"novatec-service-dependency-graph-panel"#"novatec-sdg-panel"#' $FILENAME
              fi
              if [ "$key" = 'node-exporter-full' ] || [ "$key" = 'alertmanager' ]; then
                  # The ConfigMap "grafana-dashboard-..." is invalid: metadata.annotations: Too long: must have at most 262144 bytes
                  kubectl get configmap --namespace monitoring grafana-dashboard-$key > /dev/null 2>&1 && \
                      kubectl create configmap --namespace monitoring grafana-dashboard-$key --from-file=$FILENAME --dry-run=client -o yaml | kubectl replace -f - || \
                      kubectl create configmap --namespace monitoring grafana-dashboard-$key --from-file=$FILENAME
              else
                  kubectl create configmap --namespace monitoring grafana-dashboard-$key --from-file=$FILENAME --dry-run=client -o yaml | kubectl apply -f -
              fi
          done
          # now setting up the actual default monitoring; --force might be required during upgrades
          kubectl apply --force -f manifests/
          cd -
          # per-namespace adjustments
          for ((i = 0 ; i < ${COUNTMAX} ; i++)); do
              echo "VM#: $i"
              #RBAC: per default only the following namespaces will be enabled for ServiceMonitor usage: default, kube-system, monitoring
              kubectl -n user-${i} apply -f observability/rbac.yaml
              # collect the items in this namespace that should be covered by blackbox monitoring
              for item in $BLACKBOX_TCP; do
                  BLACKBOX_TCP_FILL+=" $(echo "$item" | sed -e "s#:#.user-${i}:#")"
              done
          done # per-namespace adjustments
          #ElasticSearch Stuff
          # uses its own namespace "elastic-system"
          kubectl apply -f https://download.elastic.co/downloads/eck/1.2.1/all-in-one.yaml
          kubectl apply -f observability/elasticsearch.yaml
          until kubectl get elasticsearch exercises --namespace logging | grep --quiet Ready ; do sleep 1; date; done
          #install kibana
          kubectl apply -f observability/kibana.yaml
          until kubectl get kibana exercises --namespace logging | grep --quiet green ; do sleep 1; date; done
          #install fluentd deamonset:
          ELASTICPASSWORD=$(kubectl get secret --namespace logging exercises-es-elastic-user --output go-template='{{.data.elastic | base64decode}}')
          sed -e "s#ELASTICPASSWORD#$ELASTICPASSWORD#" observability/fluentd.yaml | kubectl apply -f -
          kubectl patch svc exercises-kb-http -p '{"spec": {"ports": [{"port": 5601,"targetPort": 5601,"name": "kibana"}],"type": "LoadBalancer"}}' --namespace logging
          until kubectl get svc --namespace logging exercises-kb-http -o jsonpath="{.status.loadBalancer.ingress[0].ip}" | grep -v --silent pending ; do sleep 1; date; done
          IP=$(kubectl get svc --namespace logging exercises-kb-http -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
          sleep 40
          curl --user "elastic:$ELASTICPASSWORD" --insecure "https://$IP:5601/api/saved_objects/index-pattern/exercises" --header 'kbn-xsrf: true' --header 'Content-Type: application/json' --data "@$SYSTEM_DEFAULTWORKINGDIRECTORY/_infrastructure/configurationAsCode/data/indexpattern.json" 
          #install jaeger standalone:
          kubectl apply -n monitoring -f observability/jaeger.yaml
          PROMETHEUS_ADDITIONAL="$(
              {
                  cat observability/prometheus-additional.yaml
                  cat observability/prometheus-additional-blackbox-tcp.yaml
                  for i in $BLACKBOX_TCP_FILL; do
                      echo "    - $i"
                  done
              } | base64 -w0
          )"
          cat << EOF | kubectl --namespace=monitoring apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: additional-scrape-configs
          type: Opaque
          data:
            prometheus-additional.yaml: $PROMETHEUS_ADDITIONAL
          EOF
          status=$?
              [ $status -eq 0 ] && echo "Provision for observability in AKS run successful" || exit $status
  
  aks-stop:
    needs: [configure-participant-pods]
    environment: automatic-aks
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
            
      - name: Azure CLI script
        uses: azure/CLI@v1
        with:
          azcliversion: 2.30.0
          inlineScript: |
            az aks stop --name "${{ env.TF_VAR_labname }}aks" --resource-group "${{ env.TF_VAR_labname }}" 

  aks-start:
    needs: [aks-stop]
    environment: automatic-aks
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Azure CLI script
        uses: azure/CLI@v1
        with:
          azcliversion: 2.30.0
          inlineScript: |
            az aks start --name "${{ env.TF_VAR_labname }}aks" --resource-group "${{ env.TF_VAR_labname }}"

  manual-aks-stop:
    environment: manual-aks-start
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Azure CLI script
        uses: azure/CLI@v1
        with:
          azcliversion: 2.30.0
          inlineScript: |
            az aks stop --name "${{ env.TF_VAR_labname }}aks" --resource-group "${{ env.TF_VAR_labname }}"

  manual-aks-start:
    environment: manual-aks-start
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Azure CLI script
        uses: azure/CLI@v1
        with:
          azcliversion: 2.30.0
          inlineScript: |
            az aks start --name "${{ env.TF_VAR_labname }}aks" --resource-group "${{ env.TF_VAR_labname }}"
  
  manual-az-destroy:
    environment: manual-az-destroy
    runs-on: ubuntu-latest
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Azure CLI script
        uses: azure/CLI@v1
        env: 
          blob: '${{ github.event.inputs.labName }}-components.tfstate'
        with:
          azcliversion: 2.30.0
          inlineScript: |
            az group delete --name "${{ github.event.inputs.labname }}" --yes
            az storage blob lease break -b terraform.tfstate --container-name terraformstate --account-name ${{ github.event.inputs.storageAccountName }}
            az storage blob delete --delete-snapshots include --account-name ${{ github.event.inputs.storageAccountName }} --container-name terraformstate --name ${blob} 